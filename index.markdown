---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

# layout: page
# layout: default
layout: single 
classes: wide
title: Welcome
author_profile: true
permalink: /index/
---
I am a Machine Learning scientist at Amazon AWS AI working on research and development of efficient optimization algorithms, large batch training and distributed training methods for pre-training/fine-tuning large language models.   
{: style="font-size:0.8em;"}

My research interests lie in Large Scale Machine Learning, Optimization, Distributed Learning, and applications to real-world problems in NLP, Ranking & Recommender Systems and Information Retrieval. I am also interested in efficient parameter estimation techniques for bayesian models. I obtained my PhD in Computer Science from UC Santa Cruz working with Prof. S.V.N. Vishwanathan on hybrid-parallel and de-centralized stochastic optimization algorithms for large-scale machine learning models. Before that I received my Masters in Computer Science from Georgia Tech. 
{: style="font-size:0.8em;"}

<!--
[Here's my CV](/files/params-cvvv.pdf).
-->

<!--
Outside work, I love all forms of outdoor activities and sports most favorites being swimming, tennis and beach volleyball. In the winters, I have enjoyed skiing on the beautiful slopes of Homewood/Heavenly at Tahoe!
-->
  

***Internship opportunities in our group***  
We have exciting opportunities for PhD student interns to work on projects related to optimization, large-scale training of deep learning models, LLMs. If you are interested, please get in touch with me.
{: style="font-size:0.8em;"}

<br>
<h2 align="left"><font color="#FF0000">News</font></h2>
<table class='news-table'>
    <col width="15%">
    <col width="85%">
    <tr>
        <td valign="top"><strong>[Apr 2024]</strong></td>
        <td> Three of our recent works <a href="https://arxiv.org/abs/2401.08893">MADA: Meta-Adaptive Optimizers through hyper-gradient Descent</a>, <a href="https://arxiv.org/abs/2404.10575">EMC^2: Efficient MCMC Negative Sampling for Contrastive Learning with Global Convergence</a> and <a href="https://arxiv.org/abs/2404.08080">Variance-reduced Zero Order Optimization for LLM Fine-tuning</a> got accepted to <a href="https://icml.cc/Conferences/2024/Dates">ICML 2024</a>. 
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[Apr 2024]</strong></td>
        <td>Pre-print of our paper, <a href="https://arxiv.org/abs/2404.10630">HLAT: High-quality Large Language Model Pre-trained on AWS Trainium</a>,
        is available on ArXiv. 
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[Apr 2024]</strong></td>
        <td>Pre-print of our paper, <a href="https://arxiv.org/abs/2404.10575">EMC2: Efficient MCMC Negative Sampling for Contrastive Learning with Global Convergence</a>,
        is available on ArXiv. 
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[Apr 2024]</strong></td>
        <td>Pre-print of our paper, <a href="https://arxiv.org/abs/2404.08080">Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models</a>,
        is available on ArXiv. 
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[Jan 2024]</strong></td>
        <td>Our paper, <a href="https://arxiv.org/abs/2401.03058">Krylov Cubic Regularized Newton: A Subspace Second-Order Method with Dimension-Free Convergence Rate</a>,
        has been accepted to 
        <a href="https://aistats.org/aistats2024/index.html">AISTATS, 2024.</a> 
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[Jan 2024]</strong></td>
        <td>Pre-print of our paper, <a href="https://arxiv.org/abs/2401.08893">MADA: Meta-Adaptive Optimizers through hyper-gradient Descent</a>,
        is available on ArXiv. 
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[Jan 2024]</strong></td>
        <td>Elevated to grade of Senior IEEE member.</td>
    </tr>
    <tr>
        <td valign="top"><strong>[Feb 2023]</strong></td>
        <td>Updated pre-print version of our work, <a href="https://arxiv.org/abs/2312.08538">Contractive error feedback for gradient compression</a>,
        is available on ArXiv. 
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[Jul 2020]</strong></td>
        <td>Received certificate of appreciation for contributions as a reviewer for <a href="https://icml.cc/Conferences/2020">ICML 2020</a> to be hosted in Vienna. 
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[Apr 2020]</strong></td>
        <td>Joined as an Applied Scientist at Amazon. 
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[Apr 2020]</strong></td>
        <td>Pre-print version of our work on Scalable Factorization Machines, <a href="https://paramsraman.github.io/files/dsfacto.pdf">DS-FACTO: Doubly Separable Factorization Machines</a>, is available on ArXiv. 
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[Jan 2020]</strong></td>
        <td>Delivered a talk on <a href="https://paramsraman.github.io/files/mlr-kdd19.pdf">Scaling Multinomial Logistic Regression through Hybrid-Parallelism</a> at <a href="https://www.fiddler.ai/">Fiddler.ai</a>, Palo Alto.
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[Jan 2020]</strong></td>
        <td>Delivered a talk on work done during my PhD thesis <a href="https://paramsraman.github.io/files/params_phd_thesis.pdf">Hybrid-Parallel Parameter Estimation for Bayesian and Frequentist Models</a> at IBM Research, Almaden.
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[Dec 2019]</strong></td>
        <td>Defended my PhD dissertation titled <a href="https://paramsraman.github.io/files/params_phd_thesis.pdf">Hybrid-Parallel Parameter Estimation for Bayesian and Frequentist Models</a>. 
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[Apr 2019]</strong></td>
        <td>Our paper <a href="https://paramsraman.github.io/files/mlr-kdd19.pdf">Scaling Multinomial Logistic Regression via Hybrid Parallelism</a>,
        has been accepted to 
        <a href="https://www.kdd.org/kdd2019/">KDD, 2019.</a> as a <font color="red">Oral Presentation (9.16% acceptance rate)</font>.
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[Dec 2018]</strong></td>
        <td>Our paper on scaling inference for mixture of exponential family models titled <a href="https://paramsraman.github.io/files/mlr-kdd19.pdf">Extreme Stochastic Variational Inference: Distributed and Asynchronous</a>,
        has been accepted to 
        <a href="https://aistats.org/aistats2019/">AISTATS, 2019.</a>
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[May 2018]</strong></td>
        <td>Invited to attend <a href="https://ifds.wisc.edu/workshops/fundamentals/">TRIPODS Madison summer school 2018 on "Fundamentals of Data Analysis</a>,
        at University of Wisconsin, Madison. 
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[May 2018]</strong></td>
        <td>Received NSF travel award to MLSE 2018. Invited to present a poster at the <a href="https://events.mcs.cmu.edu/mlse/electrical-and-computer-engineering-program/">CMU-Georgia Tech Symposium on Machine Learning in Science and Engineering</a>,
        at CMU, Pittsburgh. 
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[May 2015]</strong></td>
        <td>Attending AT&T Machine Learning Summit hosted by AT&T Research, New York. The summit will feature talks and roundtable discussions on Applications in Intelligent Systems, Big Data, and Security. 
        </td>
    </tr>
    <tr>
        <td valign="top"><strong>[Sep 2014]</strong></td>
        <td>Our work titled <a href="http://papers.nips.cc/paper/5363-ranking-via-robust-binary-classification.pdf">Ranking via Robust Binary Classification</a>,
        got accepted to 
        <a href="https://neurips.cc/Conferences/2014">NeurIPS, 2014 (19.79% acceptance rate).</a>
        </td>
    </tr>
</table>

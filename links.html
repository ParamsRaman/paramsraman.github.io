---
layout: default
section: links 
title: "Parameswaran Raman: Useful Links"
---

<h2> Useful Links</h2>
<p align="justify">
<b>Handy References:</b> <br>
        <a href="http://orion.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">-The Matrix Cookbook</a> (All you need to refresh your Matrix Calculus!) <br>
        <a href="http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">-Introduction to Conjugate Gradient Method w/o the Agonizing Pain</a><br>     
        -Cool LaTeX symbol classifier <a href="http://detexify.kirelabs.org/classify.html">tool</a><br>
        -Miscellaneous cheat sheets for quick refs <a href="static/pub/unix.txt">[Unix]</a> <a href="static/pub/git.txt">[Git]</a> <a href="">[Python]</a>
        <br><br>
        
        <b>Some interesting blogs for data mining/machine learning:</b><br>
        -Edwin Chen's <a href="http://blog.echen.me/">Blog</a><br>
        -John Langford's <a href="http://hunch.net/">Blog</a><br>
        -<a href="http://fastml.com/">FastML</a><br>
        -<a href="http://www.walkingrandomly.com/">Walking Randomly</a><br>
        -<a href="http://nuit-blanche.blogspot.it/">Nuit Blanche</a><br>
        -<a href="https://blogs.princeton.edu/imabandit/">I'm a bandit</a><br>
        <!---Bayesian or Frequentist? The never ending debate! <a href="http://cs.stanford.edu/~jsteinhardt/stats-essay.pdf">[1]</a> <a href="http://www.austincc.edu/mparker/stat/nov04/talk_nov04.pdf">[2]</a> <a href="http://arxiv.org/pdf/1208.2141v1.pdf">[3]</a> <a href="http://www.stat.ufl.edu/archived/casella/Talks/BayesRefresher.pdf">[4]</a>-->
        <br><br>
        
        <b>Random:</b><br>
        -<i>"Your algorithm can't resist the plague of overfitting!" :)</i> <a href="https://www.youtube.com/watch?v=DQWI1kvmwRg">Overfitting Thriller!</a><br>
        -The famous and amazingly well written - <a href="http://pgbovine.net/PhD-memoir.htm">PhD Grind </a> by Philip J. Guo <br>
        -<a href="http://research.microsoft.com/en-us/people/milads/"> Milos Shokouhi </a> from MSR Cambridge gave a wonderful talk on how to get the most out of your PhD (titled: Recipes for PhD), <a href="./resources/recipesforphd.pdf">Slides </a><br>
        -<a
            href="https://github.com/lixin4ever/Conference-Acceptance-Rate">Statistics
        of acceptance rates of main AI conferences</a><br>
        <!---Correlation, Causation or Coincidence!!?? <a href="http://tylervigen.com/">Check this out (video at bottom of the page)</a> <br><br>-->
         <br>
         <!---I have had a great regard for <a href="http://en.wikipedia.org/wiki/Sanskrit">Sanskrit</a> as a language and even more after I learned its possible connection to Artificial Intelligence. One of the earliest works mentioning this was by Rick Briggs in his <a href="http://www.aaai.org/ojs/index.php/aimagazine/article/view/466">paper</a> way back in 1985. A more detailed <a href="http://psychedelicjunction.com/2013/01/09/nasa-sanskrit-program-artificial-intelligence/"> article </a>describes some of the powerful features of this ancient language. Hopefully at some point, I will explore more along these directions..-->
        <br><br>
        
        <b>Recommended Books:</b><br>
        <table style="border:1px solid silver;">
            <tr style="border:1px solid silver;"><td width="600"><b>"Pattern Recognition and Machine Learning"</b> - <i>By Christopher M Bishop</i>:<br> Although vast, it covers pretty much every basic topic that one could possibly imagine in this area. The author follows a Bayesian approach to most methods with some frequentist treatment at places. With a considerable background in linear algebra, multivariate calculus and probability, this book will deliver a lot. To sum it up, I consider this book as a good rigorous graduate level introduction to machine learning! </td><td><img float="left" src="static/pub/prml-book.jpg" height="100" width="80"></td></tr>
            <tr style="border:1px solid silver;"><td width="600"><b>"Linear and Nonlinear Optimization"</b> - <i>By Igor Griva, Stephen G Nash, Ariela Sofer</i>:<br> Great introductory book on Optimization Methods. The book is semi-rigorous primarily focussing on delivering the intuition behind various methods rather than introducing terminologies right away! Very easy to read and follow. Probably, a pre-cursor to Nocedal&Wright in my opinion.</td><td><img float="left" src="static/pub/griva-sofer-nash-book.png" height="100" width="80"></td></tr>
            <tr style="border:1px solid silver;"><td width="600"><b>"Introduction to Probability"</b> - <i>By Dimitri P. Bertsekas, John N. Tsitsiklis</i>:<br> This is a great introductory textbook for probability. Dimitri is a wonderful teacher and his <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/video-lectures/">video lectures</a> are a great accompaniment to this textbook.</td><td><img float="left" src="static/pub/prob_book.jpg" height="100" width="80"></td></tr>
            <tr style="border:1px solid silver;"><td width="600"><b>"Linear Algebra and its Applications"</b> - <i>By Gilbert Strang</i>:<br> It would not be incorrect to say that anyone who has learnt linear algebra has been influenced by Gilber Strang's material in some way or the other. His explanations are so perfect that it is hard not to get the concepts! Strongly recommend watching his <a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/">video lectures</a> (freely available online) as well.</td><td><img float="left" src="static/pub/gilbert_strang.jpg" height="100" width="80"></td></tr>
        </table>
        <br>
        Great set of <a href="https://www.youtube.com/channel/UC7gOYDYEgXG1yIH_rc2LgOw/playlists">introductory pre-requisite material</a> to review before an introductory graduate course in machine learning (by <a href="https://people.eecs.berkeley.edu/~aramdas/">Aaditya Ramdas</a>).
        <br><br>
        
        <!--small>(Will be updated soon..)</small-->
    </p>

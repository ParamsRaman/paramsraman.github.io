---
layout: default
section: home
title: "Parameswaran Raman: Homepage"
---
<!-- Parameswaran Raman UCSC UC Santa Cruz Params Raman Univeristy of California
  Santa Cruz http://people.ucsc.edu/~praman1 -->
<div class="row">
  <div class="col-sm-3 col-md-2">
    <img src="static/params_pongal.png" class="img-thumbnail" width="200px"
    style="margin-top: 10px; margin-bottom: 15px">
  </div>
  <div class="col-sm-9 col-md-10">
    <h1 style="font-size: 25px">Parameswaran Raman</h1>
    <p>
    <span style="color: #000000; font-family: Helvetica Neue, Helvetica, sans-serif;
      font-weight:400">
      Applied Scientist<br>
      Amazon - AWS AI<br>
      email: prraman AT amazon DOT com 
    </span>
    </p>
  </div>
</div>

<!--p><span style="background: #FFFF33">*** UPDATE: I am currently actively looking for full-time positions in the industry!</span><br-->

<!--h2>Bio</h2-->
<p align="justify"> I am an Applied scientist in the AWS Deep Engine-Science
group. I work on distributed training for large scale transformer models.<br>

My research interests are in Large Scale Machine Learning, Optimization,
Distributed Learning, and applying them to real-world problems in applied areas
such as Ranking & Recommender Systems, Information Retrieval and NLP. I am also
interested in efficient parameter estimation for bayesian models. I obtained my
PhD in Computer Science from UC Santa Cruz working with 
<a href="http://www.stat.purdue.edu/~vishy"> Prof. S.V.N. Vishwanathan </a>
on hybrid-parallel and de-centralized stochastic optimization algorithms for large-scale machine learning models. Before
that I received my Masters in Computer Science from Georgia Tech.<br><br>

<!--br><br>
My PhD thesis work has focussed on:
<ul>
  <li> Developing reformulations for a wide spectrum of frequentist and bayesian models 
    to help distribute computation across machines more efficiently (de-centralize both data as well as
    model parameters simultaneously to achieve <i>Hybrid Parallelism</i>) by using novel algorithmic/statistical/computational techniques, 
  <li> Developing and implementing "asynchronous" distributed stochastic optimizers to solve the reformulations.
</ul-->

I am very excited about applying advances from algorithms/numerical linear
algebra and systems/high-performance computing areas to develop computationally
efficient machine learning algorithms that can deal with massive
datasets.<br><br>

<h2>Experience</h2>
<ul>
  <li style="padding: 5px 5px;"> 
    Applied Scientist Intern, Amazon AI, Palo Alto (Summer 2017)<br> 
  </li>
  <li style="padding: 5px 5px;"> 
    Research Intern, Adobe Research, San Jose (Summer 2016)<br>
  </li>
  <li style="padding: 5px 5px;"> 
    Research Intern, Microsoft Research (Cloud Information and Services
      Lab), Mountain View (Summer 2015)<br>
  </li>
  <li style="padding: 5px 5px;"> 
    Research Intern, LinkedIn (SNA), Mountain View (Summer 2014)<br>
  </li>
  <li style="padding: 5px 5px;"> 
    Software Engineer, Yahoo!, Sunnyvale (July 2011 - July 2013) <br>
  </li>
  <li style="padding: 5px 5px;"> 
    Graduate Research Assistant, Sonification Lab, Georgia Tech, Atlanta (Aug 2009 - May 2011) <br>
  </li>
  <li style="padding: 5px 5px;"> 
    Application Developer, ThoughtWorks, Bangalore (June 2008 - July
    2009)<br>
  </li>
</ul>

<p> More details can be found in my <a
    href="static/pub/params-cv.pdf" style="color:#08c">CV</a>. <br> 
<!--span>An overview of my research can be found <a href="static/pub/research-proposal.pdf">here (long version)</a> and <a href="static/pub/research-summary.pdf">here (short version)</a>. </span-->
</p>

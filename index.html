---
layout: default
section: home
title: "Parameswaran Raman: Homepage"
---
<!-- Parameswaran Raman UCSC UC Santa Cruz Params Raman Univeristy of California
  Santa Cruz http://people.ucsc.edu/~praman1 -->
<div class="row">
  <div class="col-sm-3 col-md-2">
    <img src="static/params_pongal.png" class="img-thumbnail" width="200px"
    style="margin-top: 10px; margin-bottom: 15px">
  </div>
  <div class="col-sm-9 col-md-10">
    <h1 style="font-size: 25px">Parameswaran Raman</h1>
    <p>
    <span style="color: #000000; font-family: Helvetica Neue, Helvetica, sans-serif;
      font-weight:400">
      Applied Scientist<br>
      Amazon - AWS AI<br>
      email: prraman AT amazon DOT com 
    </span>
    </p>
  </div>
</div>

<!--p><span style="background: #FFFF33">*** UPDATE: I am currently actively looking for full-time positions in the industry!</span><br-->

<!--h2>Bio</h2-->
<p align="justify"> I am an Applied scientist in the AWS AI Deep Engine-Science
group. I work on distributed training for large scale transformer models.<br><br>

My research interests are in Large Scale Machine Learning, Optimization,
Distributed Learning, and applying them to real-world problems in applied areas
such as Ranking & Recommender Systems, Information Retrieval and NLP. I am also
interested in efficient parameter estimation techniques for bayesian models. I obtained my
PhD in Computer Science from UC Santa Cruz working with 
<a href="http://www.stat.purdue.edu/~vishy"> Prof. S.V.N. Vishwanathan </a>
on hybrid-parallel and de-centralized stochastic optimization algorithms for large-scale machine learning models. Before
that I received my Masters in Computer Science from Georgia Tech.<br><br>

<!--br><br>
My PhD thesis work has focussed on:
<ul>
  <li> Developing reformulations for a wide spectrum of frequentist and bayesian models 
    to help distribute computation across machines more efficiently (de-centralize both data as well as
    model parameters simultaneously to achieve <i>Hybrid Parallelism</i>) by using novel algorithmic/statistical/computational techniques, 
  <li> Developing and implementing "asynchronous" distributed stochastic optimizers to solve the reformulations.
</ul-->

In my work I apply advances from algorithms, systems and HPC areas
to develop computationally efficient machine learning algorithms that can 
deal with massive datasets and model sizes.<br><br>

<h3>Employment </h3>
<ul>
  <li style="padding: 5px 5px;"> 
    Applied Scientist, Amazon, Palo Alto (April 2020 - Present)<br> 
  </li>
  <li style="padding: 5px 5px;"> 
    Applied Scientist Intern, Amazon AI, Palo Alto (Summer 2017)<br> 
  </li>
  <li style="padding: 5px 5px;"> 
    Research Intern, Adobe Research, San Jose (Summer 2016)<br>
  </li>
  <li style="padding: 5px 5px;"> 
    Research Intern, Microsoft Research (Cloud Information and Services
      Lab), Mountain View (Summer 2015)<br>
  </li>
  <li style="padding: 5px 5px;"> 
    Research Intern, LinkedIn (SNA), Mountain View (Summer 2014)<br>
  </li>
  <li style="padding: 5px 5px;"> 
    Software Engineer, Yahoo!, Sunnyvale (July 2011 - July 2013) <br>
  </li>
  <li style="padding: 5px 5px;"> 
    Graduate Research Assistant, Georgia Tech, Atlanta (Aug 2009 - May 2011) <br>
  </li>
  <li style="padding: 5px 5px;"> 
    Application Developer, ThoughtWorks, Bangalore (June 2008 - July
    2009)<br>
  </li>
</ul>

<p> More details can be found in my <a
    href="static/pub/params-cv.pdf" style="color:#08c">CV</a>. <br> 
<!--span>An overview of my research can be found <a href="static/pub/research-proposal.pdf">here (long version)</a> and <a href="static/pub/research-summary.pdf">here (short version)</a>. </span-->
</p>

<h3> PhD Thesis </h3>
<ul>
  <li class="paper">
    <span class="title">
      Hybrid-Parallel Parameter Estimation for Bayesian and Frequentist Models
      <!--a href="https://www.slideshare.net/secret/ozBtkzphmZTRNI"
        style="color:#08c">[pdf]</a-->
      <a href="static/pub/params_phd_thesis.pdf" style="color:#08c">[Thesis]</a>
      <a href="https://www.slideshare.net/secret/ozBtkzphmZTRNI" style="color:#08c">[Slides]</a><br>
    </span>
    <!--input type="button" id="absbutton2" value="click to read abstract"><br>
    <p id="abstract2" style="display:none;"><small>Distributed parameter
      estimation algorithms in machine learning follow two main flavors: data parallel, where the data is
      distributed across multiple workers and model parallel, where the model
      parameters are partitioned across multiple workers. The main limitation of
      the first approach is that the model parameters need to be replicated on
      every machine. This is problematic when the number of parameters is very
      large, and hence cannot fit in a single machine. This drawback of the
      latter approach is that the data needs to be replicated on each machine.
      In this thesis, I will present Hybrid-Parallelism, an approach that allows
      us to partition both, the data as well as the model parameters
      simultaneously. As a result, each worker only needs access to a subset of
      the data and a subset of the parameters while performing parameter
      updates. I will present case studies concerning popular models: (1)
      Multinomial Logistic Regression (2) Mixture of Exponential Families,
      (3) Latent Collaborative Retrieval, and (4) Factorization Machines. In all cases, 
      I will show how to exploit the access pattern of parameter updates to derive Hybrid-Parallel asynchronous algorithms.
    </small></p-->
    </li>
</ul>

<h3> Open Source Software </h3>
<ul>
  <li class="paper">
    <a href="https://bitbucket.org/params/dsmlr"
      style="color:#08c"> DS-MLR: </a> Hybrid-Parallel stochastic optimization algorithm for Multinomial
    Logistic Regression with large number of data points and large number of
    classes.
  </li>
  <li class="paper">
    <a href="https://bitbucket.org/params/dmixmodels"
      style="color:#08c"> ESVI: </a> Hybrid-Parallel variational inference
    algorithm for Mixture of Exponential Family models with large number of data
    points and mixture components.
  </li>
  <li class="paper">
    <a href="https://bitbucket.org/d\_ijk\_stra/robirank"
      style="color:#08c"> RoBiRank: </a> Robust and scalable ranking algorithm
    for Large Data (both learning to rank and latent collaborative retrieval).
  </li>
</ul>
Code is released under the Apache License ver 2.0.
